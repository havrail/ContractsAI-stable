# ğŸ¤– LOCAL AI STRATEJÄ°SÄ° - Tamamen Offline Ã‡Ã¶zÃ¼m

## ğŸ“‹ GENEL BAKIÅ

Bu dokÃ¼man, **ContractsAI** iÃ§in tamamen local ve offline Ã§alÄ±ÅŸan AI Ã§Ã¶zÃ¼mÃ¼nÃ¼ detaylandÄ±rÄ±r. Cloud servislere (GPT-4, Claude, etc.) ihtiyaÃ§ duymadan %90+ doÄŸruluk hedefine ulaÅŸmak iÃ§in tasarlanmÄ±ÅŸtÄ±r.

---

## ğŸ¯ STRATEJÄ°: ÃœÃ§ KatmanlÄ± Hybrid Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KATMAN 1: Rule-Based Extraction (HÄ±zlÄ± & Kesin)      â”‚
â”‚  - Filename parsing                                    â”‚
â”‚  - Known companies DB (fuzzy matching)                 â”‚
â”‚  - Blacklist filtering                                 â”‚
â”‚  - Regex patterns                                      â”‚
â”‚  â†’ %40-50 coverage, %100 accuracy                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KATMAN 2: Fine-Tuned Local LLM (Ana Motor)           â”‚
â”‚  - Llama 3.1 8B + LoRA Adapter                        â”‚
â”‚  - Contract-specific training (100-200 Ã¶rnek)          â”‚
â”‚  - 4-bit quantization (8GB RAM)                        â”‚
â”‚  â†’ %85-92 overall accuracy                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KATMAN 3: Ensemble Validation & Human-in-Loop        â”‚
â”‚  - Multiple extraction attempts                        â”‚
â”‚  - Consistency scoring                                 â”‚
â”‚  - Low-confidence â†’ Manual review queue               â”‚
â”‚  â†’ %90-95 final accuracy                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¥ KATMAN 2: FINE-TUNED LOCAL LLM (Ã–ncelikli Ã‡Ã¶zÃ¼m)

### **Model SeÃ§imi: Llama 3.1 8B Instruct**

#### Neden Bu Model?
- âœ… **Zaten KullanÄ±mda:** LM Studio ile mevcut sisteminizde Ã§alÄ±ÅŸÄ±yor
- âœ… **Orta Boyut:** 8B parametre (4-bit quant â†’ ~5GB RAM)
- âœ… **Ä°yi Baseline:** Generic model bile %75-80 veriyor
- âœ… **Fine-tuning Friendly:** LoRA ile 1-2 saatte eÄŸitilebilir
- âœ… **Topluluk DesteÄŸi:** GeniÅŸ ekosistem, Ã§ok kaynak

#### Alternatif Modeller (Ä°htiyaÃ§ Halinde)
1. **Mistral 7B Instruct** - Llama'dan biraz daha kÃ¼Ã§Ã¼k, hÄ±zlÄ±
2. **Phi-3 Medium (14B)** - Daha bÃ¼yÃ¼k ama daha doÄŸru
3. **Qwen 2.5 7B** - Ã‡ince + Ä°ngilizce gÃ¼Ã§lÃ¼ (multilingual)

---

### **Fine-Tuning SÃ¼reci (Step-by-Step)**

#### **AdÄ±m 1: Training Data HazÄ±rlama**

##### 1.1 Manuel Etiketleme (100-200 SÃ¶zleÅŸme)
```python
# data/training_data.jsonl

{"input": "CONTRACT: This Master Services Agreement...", 
 "output": {
   "contract_name": "Master Services Agreement",
   "signing_party": "ABC Corporation",
   "address": "123 Main St, New York, NY 10001",
   "country": "USA",
   "signed_date": "2023-01-15",
   "signature_status": "Fully Signed"
}}

# ... 200 Ã¶rnek daha
```

**KaÃ§ Ã–rnek Gerekli?**
- **Minimum:** 50 sÃ¶zleÅŸme (bazÄ± iyileÅŸme)
- **Optimal:** 100-150 sÃ¶zleÅŸme (%85-90 doÄŸruluk)
- **Ä°deal:** 200+ sÃ¶zleÅŸme (%90-95 doÄŸruluk)

**Veri DaÄŸÄ±lÄ±mÄ±:**
- %40 Basit (tek taraflÄ±, temiz PDF)
- %40 Orta (Ã§ok sayfalÄ±, scan edilmiÅŸ)
- %20 Zor (karmaÅŸÄ±k, Ã§ok taraflÄ±, dÃ¼ÅŸÃ¼k kalite)

##### 1.2 Otomatik Data Augmentation
```python
# scripts/augment_training_data.py

def augment_contract_data(sample):
    """Mevcut Ã¶rnekleri Ã§oÄŸalt"""
    
    variations = []
    
    # 1. Tarih formatÄ± varyasyonlarÄ±
    original = sample['output']['signed_date']
    variations.append({
        **sample,
        'input': sample['input'].replace(original, "15/01/2023")
    })
    
    # 2. Ä°sim varyasyonlarÄ± (bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf)
    # 3. Adres formatlarÄ±
    # ...
    
    return variations

# 100 Ã¶rnek â†’ 300 Ã¶rnek (3x augmentation)
```

---

#### **AdÄ±m 2: LoRA Fine-Tuning**

LoRA (Low-Rank Adaptation) = Modelin sadece kÃ¼Ã§Ã¼k bir kÄ±smÄ±nÄ± eÄŸit, tÃ¼m parametreleri deÄŸil.

**Avantajlar:**
- âš¡ **HÄ±zlÄ±:** 1-2 saat (vs 1-2 gÃ¼n full fine-tuning)
- ğŸ’¾ **Az Veri:** 50-100 Ã¶rnek yeterli
- ğŸ’° **Ucuz:** Normal GPU (RTX 3060) yeterli
- ğŸ“¦ **KÃ¼Ã§Ã¼k:** Adapter dosyasÄ± ~100MB (base model 5GB deÄŸiÅŸmez)

##### 2.1 Environment Setup
```bash
# 1. Unsloth yÃ¼kle (en hÄ±zlÄ± fine-tuning library)
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --upgrade --no-deps xformers trl peft accelerate bitsandbytes

# veya Google Colab kullan (Ã¼cretsiz GPU)
```

##### 2.2 Training Script
```python
# scripts/finetune_llama.py

from unsloth import FastLanguageModel
import torch

# 1. Base model yÃ¼kle
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Meta-Llama-3.1-8B-Instruct",
    max_seq_length=4096,
    dtype=None,  # Auto-detect
    load_in_4bit=True,  # 4-bit quantization (RAM optimize)
)

# 2. LoRA configuration
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank (bÃ¼yÃ¼k = daha gÃ¼Ã§lÃ¼ ama yavaÅŸ)
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
)

# 3. Training data hazÄ±rla
from datasets import load_dataset

dataset = load_dataset("json", data_files="data/training_data.jsonl")

def formatting_func(examples):
    """Dataset'i model formatÄ±na Ã§evir"""
    texts = []
    for input_text, output_json in zip(examples['input'], examples['output']):
        text = f"""### Instruction:
Extract contract information from the following text and return JSON.

### Input:
{input_text}

### Response:
{json.dumps(output_json)}"""
        texts.append(text)
    return {"text": texts}

dataset = dataset.map(formatting_func, batched=True)

# 4. Trainer setup
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    dataset_text_field="text",
    max_seq_length=4096,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        num_train_epochs=3,  # 3 epoch yeterli
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
    ),
)

# 5. Train!
trainer.train()

# 6. Save LoRA adapter
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")

# 7. (Opsiyonel) GGUF formatÄ±na Ã§evir (LM Studio iÃ§in)
model.save_pretrained_gguf("contracts_llama_8b", tokenizer)
```

**EÄŸitim SÃ¼resi:**
- Google Colab (T4 GPU): ~1-2 saat
- RTX 3060 (12GB): ~1 saat
- RTX 4090 (24GB): ~20-30 dakika

**Maliyet:**
- Google Colab Free: $0 (limitli)
- Google Colab Pro: $10/ay (unlimited)
- Kendi GPU: $0 (bir kerelik elektrik)

---

#### **AdÄ±m 3: Model Deployment (LM Studio'ya Entegrasyon)**

##### 3.1 GGUF Model Export
```python
# Model'i GGUF formatÄ±na Ã§evir (LM Studio formatÄ±)
model.save_pretrained_gguf(
    "contracts_llama_8b",
    tokenizer,
    quantization_method="q4_k_m"  # 4-bit quantization
)

# Ã‡Ä±ktÄ±: contracts_llama_8b-Q4_K_M.gguf (~5GB)
```

##### 3.2 LM Studio'ya YÃ¼kle
```bash
# 1. Model dosyasÄ±nÄ± LM Studio models klasÃ¶rÃ¼ne kopyala
# Windows: C:\Users\<user>\.cache\lm-studio\models\

cp contracts_llama_8b-Q4_K_M.gguf "C:\Users\dagha\.cache\lm-studio\models\custom\"

# 2. LM Studio'yu aÃ§ ve model'i seÃ§
# 3. API Server'Ä± baÅŸlat (localhost:1234)
```

##### 3.3 ContractsAI Config GÃ¼ncelle
```python
# config.py

# Yeni fine-tuned model
LM_STUDIO_MODEL_NAME = "contracts_llama_8b-Q4_K_M"

# llm_client.py'de model adÄ±nÄ± gÃ¼ncelle
self.model_name = LM_STUDIO_MODEL_NAME
```

---

### **Beklenen Ä°yileÅŸme**

| Metrik | Baseline (Generic) | Fine-tuned | Ä°yileÅŸme |
|--------|-------------------|------------|----------|
| **Genel DoÄŸruluk** | 75% | **88-92%** | +13-17% |
| **Signing Party** | 80% | **95%** | +15% |
| **Address** | 70% | **90%** | +20% |
| **Country** | 85% | **95%** | +10% |
| **Signed Date** | 88% | **95%** | +7% |
| **Contract Name** | 65% | **85%** | +20% |

---

## ğŸ”„ SÃœREKLÄ° Ä°YÄ°LEÅME DÃ–NGÃœSÃœ

```
1. SÃ¶zleÅŸmeleri Ä°ÅŸle
        â†“
2. KullanÄ±cÄ± DÃ¼zeltmeleri Kaydet (feedback_service.py)
        â†“
3. Her 100 DÃ¼zeltmede Export Yap
   (api.py â†’ /api/export/training-data)
        â†“
4. Yeni Training Data ile Re-train
   (3 ayda bir veya 500+ dÃ¼zeltme)
        â†“
5. Yeni Model Deploy Et
        â†“
1. (Daha Ä°yi DoÄŸrulukla) Tekrar BaÅŸla
```

### **Otomatik Re-training Pipeline**

```python
# scripts/auto_retrain.py

def check_and_retrain():
    """Her hafta Ã§alÄ±ÅŸan otomatik re-training kontrolÃ¼"""
    
    feedback = FeedbackService()
    
    # Son re-training'den beri kaÃ§ dÃ¼zeltme var?
    new_corrections = get_corrections_since_last_training()
    
    if len(new_corrections) >= 100:
        logger.info(f"ğŸ“ {len(new_corrections)} yeni dÃ¼zeltme bulundu. Re-training baÅŸlÄ±yor...")
        
        # 1. Export training data
        export_corrections_to_training_data()
        
        # 2. Trigger fine-tuning (Colab notebook veya local)
        trigger_finetuning_job()
        
        # 3. Email bildirim
        send_email("Re-training completed! New model ready.")

# Cron job: Her Pazar 02:00
# 0 2 * * 0 python scripts/auto_retrain.py
```

---

## ğŸš€ ALTERNATÄ°F: Embedded Tiny Model (Gelecek Ä°Ã§in)

EÄŸer fine-tuning Ã§ok karmaÅŸÄ±k geliyorsa, daha kÃ¼Ã§Ã¼k bir model **executable iÃ§ine gÃ¶mÃ¼lebilir**.

### **Model: Phi-3 Mini (3.8B)**
- Boyut: ~2GB (quantized)
- RAM: 4GB
- DoÄŸruluk: %80-85 (fine-tune ile %88)

### **Avantaj:**
- âœ… Tek executable, harici baÄŸÄ±mlÄ±lÄ±k yok
- âœ… LM Studio gereksiz
- âœ… Ã‡ok hÄ±zlÄ± (CPU'da bile)

### **Dezavantaj:**
- âŒ Daha dÃ¼ÅŸÃ¼k doÄŸruluk (8B modelden)
- âŒ Embedding karmaÅŸÄ±k

**SonuÃ§:** Åimdilik **Fine-tuned Llama 3.1 8B + LM Studio** daha pratik.

---

## ğŸ“Š PERFORMANS TAHMÄ°NÄ°

### **Ä°ÅŸlem HÄ±zÄ± (Fine-tuned Model)**
- **Baseline:** 100 PDF = 5 dakika
- **Fine-tuned:** 100 PDF = 4 dakika (LLM daha emin, daha az retry)
- **Confidence > 85%:** %60 â†’ %80 (daha az manuel review)

### **DoÄŸruluk KazancÄ±**
```
Rule-based (40% coverage, 100% accurate)
    +
Fine-tuned LLM (60% coverage, 90% accurate)
    +
Human Review (low confidence < 15%)
    =
TOPLAM: %92-95 overall accuracy
```

---

## ğŸ› ï¸ UYGULAMA ADIMLARI (Ã–NCELÄ°K SIRASI)

### **Hafta 1: Veri HazÄ±rlama**
- [ ] 100 sÃ¶zleÅŸme seÃ§ (Ã§eÅŸitli tipte)
- [ ] Manuel etiketleme yap (JSON format)
- [ ] Data augmentation uygula (2x-3x Ã§oÄŸalt)
- [ ] Train/test split (80/20)

### **Hafta 2: Fine-tuning**
- [ ] Google Colab setup (veya local GPU)
- [ ] Unsloth + LoRA script hazÄ±rla
- [ ] Ä°lk training run (3 epoch)
- [ ] Test set'te doÄŸruluk Ã¶lÃ§
- [ ] Hyperparameter tuning (gerekirse)

### **Hafta 3: Deployment & Integration**
- [ ] GGUF export
- [ ] LM Studio'ya yÃ¼kle
- [ ] ContractsAI'de model deÄŸiÅŸtir
- [ ] 50 sÃ¶zleÅŸme ile production test
- [ ] Baseline ile karÅŸÄ±laÅŸtÄ±r

### **Hafta 4: Monitoring & Ä°yileÅŸtirme**
- [ ] Feedback loop aktifleÅŸtir
- [ ] Accuracy metrics takip et
- [ ] Problem alanlarÄ± belirle
- [ ] Prompt tuning yap
- [ ] Ä°kinci iteration planla

---

## ğŸ’¡ SORU & CEVAPLAR

### **S: Fine-tuning iÃ§in GPU ÅŸart mÄ±?**
**C:** HayÄ±r! Google Colab Free tier yeterli (T4 GPU, Ã¼cretsiz). EÄŸer premium alÄ±rsanÄ±z ($10/ay) daha hÄ±zlÄ±.

### **S: Her dÃ¼zeltmede re-train etmek gerekir mi?**
**C:** HayÄ±r. Her 100-200 dÃ¼zeltmede bir (3-6 ayda bir) yeterli.

### **S: Model boyutu Ã§ok bÃ¼yÃ¼k, kÃ¼Ã§Ã¼ltebilir miyiz?**
**C:** Evet! 4-bit quantization ile 8B model ~5GB. Daha kÃ¼Ã§Ã¼k istiyorsanÄ±z Phi-3 Mini (3.8B) kullanabilirsiniz.

### **S: LM Studio yerine direkt Python'da Ã§alÄ±ÅŸtÄ±rabilir miyiz?**
**C:** Evet! `llama-cpp-python` ile direkt entegre edilebilir. Ancak LM Studio daha user-friendly.

### **S: Fine-tuning baÅŸarÄ±sÄ±z olursa?**
**C:** Rule-based + Generic model + Human review ile de %85-88 ulaÅŸÄ±labilir. Fine-tuning bonus.

---

## ğŸ“š KAYNAKLAR

### **Fine-tuning Tutorials:**
- Unsloth Documentation: https://github.com/unslothai/unsloth
- Llama 3.1 Fine-tuning Guide: https://huggingface.co/blog/llama31
- Google Colab Template: https://colab.research.google.com/...

### **Model Hubs:**
- Hugging Face: https://huggingface.co/models
- LM Studio Compatible Models: https://lmstudio.ai/models

### **Community:**
- r/LocalLLaMA (Reddit)
- Hugging Face Forums
- Llama Community Discord

---

## âœ… SONUÃ‡

**En Ä°yi Strateji:**
1. âœ… **Rule-based extraction** (hÄ±zlÄ± kazanÃ§lar)
2. âœ… **Fine-tuned Llama 3.1 8B** (ana motor)
3. âœ… **Feedback loop** (sÃ¼rekli iyileÅŸme)
4. âœ… **Human-in-loop** (low confidence review)

**Hedef:**
- %90-95 overall accuracy
- Tamamen local & offline
- Maliyet: ~$10 (Colab Pro, opsiyonel)
- SÃ¼re: 3-4 hafta

**BaÅŸlangÄ±Ã§:** Training data hazÄ±rlama âœ¨

---

## ğŸ§© QWEN3-VL-8B-INSTRUCT GGUF ENTEGRASYON EKÄ°

### Neden Qwen3-VL-8B?
| Ä°htiyaÃ§ | Qwen VL KatkÄ±sÄ± |
|---------|-----------------|
| Ä°mza alanÄ± / mÃ¼hÃ¼r tespiti | GÃ¶rsel encoder ile zero-shot mÃ¼mkÃ¼n |
| Scan edilmiÅŸ dÃ¼ÅŸÃ¼k kalite PDF | OCR Ã¶ncesi gÃ¶rselden doÄŸrudan Ã§Ä±karÄ±m |
| Tablo veya kutu iÃ§i metin | GÃ¶rsel baÄŸlam daha tutarlÄ± yakalanÄ±r |
| Ã‡ok dilli iÃ§eriÄŸin karÄ±ÅŸÄ±mÄ± | TR + EN karÄ±ÅŸÄ±mÄ± iyi idare eder |

### Mimari Ã–neri (Hybrid Backend)
```
Quality Check (pdf_quality_checker)
     â”œâ”€ score >= 80 & text_density yÃ¼ksek â†’ Text Llama (fine-tuned)
     â”œâ”€ score < 80 veya is_scanned True â†’ Qwen3-VL-8B (vision extraction)
     â””â”€ fallback hata â†’ OCR + text model yeniden dene
```

### Backend Abstraction
`model_provider.py` ile Ã¼Ã§lÃ¼ strateji:
1. In-process `llama-cpp-python` (GGUF direkt)
2. LM Studio (OpenAI style endpoint)
3. Ollama (text aÄŸÄ±rlÄ±klÄ±, vision sÄ±nÄ±rlÄ± â†’ sadece fallback)

### Qwen VL Fine-tune Notu
- Multi-modal LoRA iÃ§in her sayfa gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ (JPEG) + hedef JSON etiketini eÅŸlemeniz gerekir.
- Ä°lk iterasyonda yalnÄ±zca TEXT tower fine-tune yeterli; vision kÄ±smÄ± zero-shot kullanÄ±labilir.
- Vision LoRA iÃ§in ek karmaÅŸÄ±klÄ±k (image patch embedding adaptasyonu). Gelecek faza bÄ±rakÄ±lmalÄ±.

### Multi-modal Dataset Format (Ã–rnek JSONL satÄ±rÄ±)
```json
{
    "page_images": ["data:image/jpeg;base64,....", "data:image/jpeg;base64,..."],
    "full_text": "CONTRACT PAGE 1...\nPAGE 2...",
    "labels": {
        "counterparty_name": "ABC Mobile",
        "address": "Street 12, Tallinn, Estonia",
        "country": "Estonia",
        "signed_date": "2023-06-15",
        "both_signed": true,
        "contract_name": "Master Services Agreement"
    }
}
```

### Ã–nerilen Yol
1. Åimdi: Llama 3.1 8B LoRA (text) + Qwen VL inference (vision sayfalarÄ±na seÃ§ici).
2. Sonra: DÃ¼zeltmelerden multimodal dataset Ã¼retimi.
3. Faz 2: Qwen VL LoRA (yalnÄ±zca kritik alanlarda - imza statÃ¼sÃ¼, adres bloklarÄ±).

### Entegrasyon Durumu
- `llm_client.py` iÃ§ine unified provider eklendi.
- `model_provider.py` backend geÃ§iÅŸi otomatik deniyor.
- Ä°leride pipeline iÃ§inde: kalite raporuna gÃ¶re `provider.chat(...)` Ã§aÄŸrÄ±sÄ± seÃ§ilecektir.

### Ã–nemli Ã‡evresel DeÄŸiÅŸkenler (.env)
```
LLM_MODEL=Qwen3-VL-8B-Instruct-GGUF
GGUF_MODEL_PATH=./models/Qwen3-VL-8B-Instruct-Q4_K_M.gguf
LM_STUDIO_IP=http://localhost:1234
OLLAMA_HOST=http://localhost:11434
LLAMA_THREADS=8
```

### Test HÄ±zlÄ± KomutlarÄ±
```bash
python src_python/model_provider.py  # basit quick_test
```

### Riskler & Mitigasyon
| Risk | Ã‡Ã¶zÃ¼m |
|------|-------|
| Vision model yavaÅŸ | Sadece dÃ¼ÅŸÃ¼k kalite sayfalarda kullan |
| Multi-modal fine-tune karmaÅŸÄ±k | Ä°lk fazda text LoRA + zero-shot vision |
| GPU bellek sÄ±nÄ±rÄ± | 4-bit quant, page baÅŸÄ±na tek gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±rÄ± |
| JSON format sapmasÄ± | Post-processing regex + doÄŸrulama katmanÄ± |

---
**Qwen VL entegrasyonu eklenmiÅŸtir. Fine-tuning fazÄ± iÃ§in Ã¶nce text LoRA uygulanacaktÄ±r.**
